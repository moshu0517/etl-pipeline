# Avazu CTR â€” Local-first, Cloud-ready ETL Pipeline

This project builds a **production-style ETL pipeline** for the Avazu Click-Through Rate dataset.  
It demonstrates the full data engineering workflow â€” **Extract â†’ Transform â†’ Validate â†’ Load (ETL)** â€” implemented in Python.

The pipeline is designed to:
- Run fully locally with reproducible outputs.
- Be cloud-ready â€” capable of uploading curated data to AWS S3 (optional, no cost required).
- Showcase clean modular structure, logging, and data quality validation.

*Author: Moshu Huang*  
*If you share or fork this project, please credit the author and link back to this repository.*

## ğŸ“ Repository Structure

The project follows a **modular, production-style** layout:

```text
ETL-Pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # Original compressed data (.gz)
â”‚   â”œâ”€â”€ samples/        # Small sample CSVs for quick testing
â”‚   â”œâ”€â”€ staged/         # Cleaned + transformed intermediate files
â”‚   â””â”€â”€ curated/        # Final curated Parquet files (optionally uploaded to S3)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py           # Path configs & runtime parameters
â”‚   â”œâ”€â”€ logging_config.py   # Centralized logging setup
â”‚   â”œâ”€â”€ extract.py          # Reads raw.gz â†’ sample CSV
â”‚   â”œâ”€â”€ transform.py        # Cleans + features â†’ staged CSV
â”‚   â”œâ”€â”€ validate.py         # Schema/nulls/ID checks
â”‚   â”œâ”€â”€ load.py             # Writes curated parquet (+ optional S3 upload)
â”‚   â””â”€â”€ pipeline.py         # Orchestrates ETL: Extractâ†’Transformâ†’Validateâ†’Load
â”‚
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ README.md               # Documentation
â””â”€â”€ .gitignore              # Ignore large/raw artifacts